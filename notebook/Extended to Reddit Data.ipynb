{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Analyzing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    This function receives comments and returns clean word-list\n",
    "    \"\"\"\n",
    "    # Convert to string\n",
    "    comment=str(comment)\n",
    "    #Convert to lower case , so that Hi and hi are the same\n",
    "#     comment=comment.lower()\n",
    "    #remove \\n\n",
    "    comment=re.sub(\"\\\\n\",\" \",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n",
    "    #removing usernames\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    # https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644\n",
    "    special_character_removal=re.compile(r'[^a-z\\?\\!\\.\\,\\' ]',re.IGNORECASE)\n",
    "    comment=special_character_removal.sub(\" \", comment)\n",
    "    \n",
    "    #Split the sentences into words\n",
    "    words=TweetTokenizer().tokenize(comment)\n",
    "    \n",
    "    # (')aphostophe  replacement (ie)   you're --> you are  \n",
    "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
    "    words=[appos[word] if word in appos else word for word in words]\n",
    "    words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "#     words = [w for w in words if not w in eng_stopwords]\n",
    "    \n",
    "    clean_sent=\" \".join(words)\n",
    "    clean_sent=re.sub(\"\\'\", \"\", clean_sent)\n",
    "    # remove any non alphanum,digit character\n",
    "    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n",
    "    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n",
    "    return(clean_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    embed_size = 128\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler from previous training\n",
    "\n",
    "def add_features(df):\n",
    "    \n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']+0.1),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.comment_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df\n",
    "\n",
    "train = train_data\n",
    "test = test_data\n",
    "\n",
    "train['comment_text'] = corpus_train\n",
    "test['comment_text'] = corpus_test\n",
    "\n",
    "train = add_features(train)\n",
    "test = add_features(test)\n",
    "\n",
    "features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, test_features)))\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)\n",
    "\n",
    "y_train = train[label_cols].values\n",
    "y_test = test_ground_truth[label_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287433\n"
     ]
    }
   ],
   "source": [
    "# tokenizer from previous training\n",
    "max_features = 20000\n",
    "maxlen = 50\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(corpus_train) + list(corpus_test))\n",
    "X_train_sequence = tokenizer.texts_to_sequences(corpus_train)\n",
    "X_test_sequence = tokenizer.texts_to_sequences(corpus_test)\n",
    "\n",
    "x_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\n",
    "print(len(tokenizer.word_index))\n",
    "\n",
    "y_train = train[label_cols].values\n",
    "y_test = test_ground_truth[label_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 52s, sys: 6.09 s, total: 2min 59s\n",
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the FastText Web Crawl vectors\n",
    "EMBEDDING_FILE_FASTTEXT=\"data/ml_models/crawl-300d-2M.vec\"\n",
    "EMBEDDING_FILE_TWITTER=\"data/ml_models/glove.twitter.27B/glove.twitter.27B.200d.txt\"\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index_ft = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'))\n",
    "embeddings_index_tw = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE_TWITTER,encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 50s, sys: 6.4 s, total: 7min 56s\n",
      "Wall time: 7min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE_FASTTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n",
    "\n",
    "words = spell_model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend the study to Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>created_time</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>theydidthefuckyou</td>\n",
       "      <td>t5_32jwi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511360e+09</td>\n",
       "      <td>Fuck him, fuck the FCC, fuck big corporate int...</td>\n",
       "      <td>2017-11-22 22:14:23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>theydidthefuckyou</td>\n",
       "      <td>t5_32jwi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511346e+09</td>\n",
       "      <td>Fuck him!</td>\n",
       "      <td>2017-11-22 18:19:27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>theydidthefuckyou</td>\n",
       "      <td>t5_32jwi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511352e+09</td>\n",
       "      <td>Fuck this guy and the high horse he rode in on</td>\n",
       "      <td>2017-11-22 20:04:24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>theydidthefuckyou</td>\n",
       "      <td>t5_32jwi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511345e+09</td>\n",
       "      <td>Fuck You\\n\\n*I am a bot, and this action was p...</td>\n",
       "      <td>2017-11-22 18:05:05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>theydidthefuckyou</td>\n",
       "      <td>t5_32jwi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511367e+09</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2017-11-23 00:08:22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          subreddit subreddit_id  num_reports  report_reasons  \\\n",
       "0           0  theydidthefuckyou     t5_32jwi          NaN             NaN   \n",
       "1           1  theydidthefuckyou     t5_32jwi          NaN             NaN   \n",
       "2           2  theydidthefuckyou     t5_32jwi          NaN             NaN   \n",
       "3           3  theydidthefuckyou     t5_32jwi          NaN             NaN   \n",
       "4           4  theydidthefuckyou     t5_32jwi          NaN             NaN   \n",
       "\n",
       "        created                                               body  \\\n",
       "0  1.511360e+09  Fuck him, fuck the FCC, fuck big corporate int...   \n",
       "1  1.511346e+09                                          Fuck him!   \n",
       "2  1.511352e+09     Fuck this guy and the high horse he rode in on   \n",
       "3  1.511345e+09  Fuck You\\n\\n*I am a bot, and this action was p...   \n",
       "4  1.511367e+09                                          [deleted]   \n",
       "\n",
       "          created_time  toxic  \n",
       "0  2017-11-22 22:14:23      1  \n",
       "1  2017-11-22 18:19:27      1  \n",
       "2  2017-11-22 20:04:24      1  \n",
       "3  2017-11-22 18:05:05      1  \n",
       "4  2017-11-23 00:08:22      1  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_toxic = pd.read_csv('data/toxic_comments_2nd.csv')\n",
    "reddit_toxic['toxic'] = 1\n",
    "reddit_toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>created_time</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HistoryMemes</td>\n",
       "      <td>t5_2v2cd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.553417e+09</td>\n",
       "      <td>*D I D N O T C O N T A I N A S I N G L E D R O...</td>\n",
       "      <td>2019-03-24 16:49:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>HistoryMemes</td>\n",
       "      <td>t5_2v2cd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.553415e+09</td>\n",
       "      <td>r/cursedgifs</td>\n",
       "      <td>2019-03-24 16:13:52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>HistoryMemes</td>\n",
       "      <td>t5_2v2cd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.553417e+09</td>\n",
       "      <td>This is a masterpiece</td>\n",
       "      <td>2019-03-24 16:43:38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HistoryMemes</td>\n",
       "      <td>t5_2v2cd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.553422e+09</td>\n",
       "      <td>HEART THE SIZE OF A PEPPERCORN</td>\n",
       "      <td>2019-03-24 18:08:48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>HistoryMemes</td>\n",
       "      <td>t5_2v2cd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.553415e+09</td>\n",
       "      <td>⣿⣷⡶⠚⠉⢀⣤⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠋⠠⣴⣿⣿⣿⣿⣶⣤⣤⣤ ⠿⠥⢶⡏⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿...</td>\n",
       "      <td>2019-03-24 16:17:31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     subreddit subreddit_id  num_reports  report_reasons  \\\n",
       "0           0  HistoryMemes     t5_2v2cd          NaN             NaN   \n",
       "1           1  HistoryMemes     t5_2v2cd          NaN             NaN   \n",
       "2           2  HistoryMemes     t5_2v2cd          NaN             NaN   \n",
       "3           3  HistoryMemes     t5_2v2cd          NaN             NaN   \n",
       "4           4  HistoryMemes     t5_2v2cd          NaN             NaN   \n",
       "\n",
       "        created                                               body  \\\n",
       "0  1.553417e+09  *D I D N O T C O N T A I N A S I N G L E D R O...   \n",
       "1  1.553415e+09                                       r/cursedgifs   \n",
       "2  1.553417e+09                              This is a masterpiece   \n",
       "3  1.553422e+09                    HEART THE SIZE OF A PEPPERCORN    \n",
       "4  1.553415e+09  ⣿⣷⡶⠚⠉⢀⣤⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠋⠠⣴⣿⣿⣿⣿⣶⣤⣤⣤ ⠿⠥⢶⡏⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿...   \n",
       "\n",
       "          created_time  toxic  \n",
       "0  2019-03-24 16:49:22      0  \n",
       "1  2019-03-24 16:13:52      0  \n",
       "2  2019-03-24 16:43:38      0  \n",
       "3  2019-03-24 18:08:48      0  \n",
       "4  2019-03-24 16:17:31      0  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_normal = pd.read_csv('data/reddit_data.csv')\n",
    "reddit_normal['toxic'] = 0\n",
    "reddit_normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>created_time</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>theydidthefuckyou</td>\n",
       "      <td>t5_32jwi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511360e+09</td>\n",
       "      <td>Fuck him, fuck the FCC, fuck big corporate int...</td>\n",
       "      <td>2017-11-22 22:14:23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>theydidthefuckyou</td>\n",
       "      <td>t5_32jwi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511346e+09</td>\n",
       "      <td>Fuck him!</td>\n",
       "      <td>2017-11-22 18:19:27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>theydidthefuckyou</td>\n",
       "      <td>t5_32jwi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511352e+09</td>\n",
       "      <td>Fuck this guy and the high horse he rode in on</td>\n",
       "      <td>2017-11-22 20:04:24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>theydidthefuckyou</td>\n",
       "      <td>t5_32jwi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511345e+09</td>\n",
       "      <td>Fuck You\\n\\n*I am a bot, and this action was p...</td>\n",
       "      <td>2017-11-22 18:05:05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>theydidthefuckyou</td>\n",
       "      <td>t5_32jwi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511367e+09</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2017-11-23 00:08:22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          subreddit subreddit_id  num_reports  report_reasons  \\\n",
       "0           0  theydidthefuckyou     t5_32jwi          NaN             NaN   \n",
       "1           1  theydidthefuckyou     t5_32jwi          NaN             NaN   \n",
       "2           2  theydidthefuckyou     t5_32jwi          NaN             NaN   \n",
       "3           3  theydidthefuckyou     t5_32jwi          NaN             NaN   \n",
       "4           4  theydidthefuckyou     t5_32jwi          NaN             NaN   \n",
       "\n",
       "        created                                               body  \\\n",
       "0  1.511360e+09  Fuck him, fuck the FCC, fuck big corporate int...   \n",
       "1  1.511346e+09                                          Fuck him!   \n",
       "2  1.511352e+09     Fuck this guy and the high horse he rode in on   \n",
       "3  1.511345e+09  Fuck You\\n\\n*I am a bot, and this action was p...   \n",
       "4  1.511367e+09                                          [deleted]   \n",
       "\n",
       "          created_time  toxic  \n",
       "0  2017-11-22 22:14:23      1  \n",
       "1  2017-11-22 18:19:27      1  \n",
       "2  2017-11-22 20:04:24      1  \n",
       "3  2017-11-22 18:05:05      1  \n",
       "4  2017-11-23 00:08:22      1  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_data = pd.concat([reddit_toxic, reddit_normal], axis=0)\n",
    "reddit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.54 s, sys: 7.03 ms, total: 3.55 s\n",
      "Wall time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = reddit_data\n",
    "# This is time-consuming. Set the boolean check if necessary\n",
    "if 0==1:\n",
    "    corpus_test = [clean(text) for text in test['body']]\n",
    "    \n",
    "    with open('data/reddit_comment_test_with_stopword.txt', 'w', encoding='utf-8') as f:\n",
    "        for comment in corpus_test:\n",
    "            f.write(comment + '\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to load model\n",
    "model = get_model(features)\n",
    "model.load_weights(\"weights.best.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['comment_text'] = corpus_test\n",
    "test = add_features(test)\n",
    "\n",
    "test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "test_features = ss.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287433\n"
     ]
    }
   ],
   "source": [
    "# For best score (Public: 9869, Private: 9865), change to max_features = 283759, maxlen = 900\n",
    "max_features = 20000\n",
    "maxlen = 50\n",
    "\n",
    "# tokenizer = text.Tokenizer(num_words=max_features)\n",
    "# tokenizer.fit_on_texts(list(corpus_train) + list(corpus_test))\n",
    "# X_train_sequence = tokenizer.texts_to_sequences(corpus_train)\n",
    "X_test_sequence = tokenizer.texts_to_sequences(corpus_test)\n",
    "\n",
    "# x_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\n",
    "print(len(tokenizer.word_index))\n",
    "\n",
    "# y_train = train[label_cols].values\n",
    "y_test = test['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words,501))\n",
    "\n",
    "something_tw = embeddings_index_tw.get(\"something\")\n",
    "something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "something = np.zeros((501,))\n",
    "something[:300,] = something_ft\n",
    "something[300:500,] = something_tw\n",
    "something[500,] = 0\n",
    "\n",
    "def all_caps(word):\n",
    "    return len(word) > 1 and word.isupper()\n",
    "\n",
    "def embed_word(embedding_matrix,i,word):\n",
    "    embedding_vector_ft = embeddings_index_ft.get(word)\n",
    "    if embedding_vector_ft is not None: \n",
    "        if all_caps(word):\n",
    "            last_value = np.array([1])\n",
    "        else:\n",
    "            last_value = np.array([0])\n",
    "        embedding_matrix[i,:300] = embedding_vector_ft\n",
    "        embedding_matrix[i,500] = last_value\n",
    "        embedding_vector_tw = embeddings_index_tw.get(word)\n",
    "        if embedding_vector_tw is not None:\n",
    "            embedding_matrix[i,300:500] = embedding_vector_tw\n",
    "\n",
    "            \n",
    "# Fasttext vector is used by itself if there is no glove vector but not the other way around.\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    if i >= max_features: continue\n",
    "        \n",
    "    if embeddings_index_ft.get(word) is not None:\n",
    "        embed_word(embedding_matrix,i,word)\n",
    "    else:\n",
    "        # change to > 20 for better score. Previously 0\n",
    "        if len(word) > 20:\n",
    "            embedding_matrix[i] = something\n",
    "        else:\n",
    "            word2 = correction(word)\n",
    "            if embeddings_index_ft.get(word2) is not None:\n",
    "                embed_word(embedding_matrix,i,word2)\n",
    "            else:\n",
    "                word2 = correction(singlify(word))\n",
    "                if embeddings_index_ft.get(word2) is not None:\n",
    "                    embed_word(embedding_matrix,i,word2)\n",
    "                else:\n",
    "                    embedding_matrix[i] = something "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23674/23674 [==============================] - 18s 762us/step\n"
     ]
    }
   ],
   "source": [
    "pred_prob = model.predict([x_test,test_features], batch_size=batch_size,verbose=1)\n",
    "preds = np.zeros((len(test), len(label_cols)))\n",
    "for i, category in enumerate(label_cols):\n",
    "    preds[:,i] = [1 if x >= 0.5 else 0 for x in pred_prob[:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.91953135e-01, 4.45396185e-01, 9.90529060e-01, 7.89734721e-03,\n",
       "        7.36517072e-01, 2.72302926e-02],\n",
       "       [9.85708833e-01, 2.15753585e-01, 9.84757602e-01, 4.60192561e-03,\n",
       "        6.45035028e-01, 1.32909417e-02],\n",
       "       [9.55524981e-01, 2.00703889e-01, 9.74683940e-01, 2.10027099e-02,\n",
       "        4.35233355e-01, 1.30805969e-02],\n",
       "       [9.14638519e-01, 7.01685548e-02, 9.53739524e-01, 1.91512704e-03,\n",
       "        4.71423686e-01, 7.42796063e-03],\n",
       "       [2.05942094e-02, 8.92996788e-04, 6.34765625e-03, 2.63875723e-03,\n",
       "        7.01850653e-03, 3.68714333e-04],\n",
       "       [5.30745685e-02, 4.95135784e-04, 1.37698352e-02, 1.66311860e-03,\n",
       "        5.38703799e-03, 7.26968050e-04],\n",
       "       [1.56222194e-01, 2.29719281e-03, 3.19269121e-01, 6.45220280e-05,\n",
       "        2.19537616e-02, 3.96072865e-04],\n",
       "       [9.98004794e-01, 7.18249917e-01, 9.94820118e-01, 4.38839197e-03,\n",
       "        8.97747636e-01, 2.72137821e-02],\n",
       "       [9.94788170e-01, 3.59553039e-01, 9.91478264e-01, 5.12626767e-03,\n",
       "        8.30015719e-01, 1.48847699e-02],\n",
       "       [9.95266259e-01, 3.59692097e-01, 9.88341570e-01, 5.66408038e-03,\n",
       "        8.85314584e-01, 1.40559375e-02]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob[:,][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fuck him , fuck the FCC , fuck big corporate interest , fuck Trump for appoint him , and ya know what ? Fuck you , too , for good measure . And fuck me , too . Please ...',\n",
       " 'Fuck him !',\n",
       " 'Fuck this guy and the high horse he ride in on',\n",
       " 'Fuck You I be a bot , and this action be perform automatically . Please contact the moderators of this subreddit message compose ? to r theydidthefuckyou if you have any question or concern .',\n",
       " 'delete',\n",
       " 'If that is a not I have live a life of privilege , but I had still like to screw over billions of ordinary folks for my corporate buddies kinda smile , then my butts version of fart the national anthem be head to the top of the country chart .',\n",
       " 'Congratulations ! Your post reach top five in r all rise . The post be thus x post r Masub comment eq ak fuck this guy rtheydidthefuckyou to r masub . It have point in minutes when the x post be make .',\n",
       " 'Fuck this fuck cunt and his fuckity fuck agenda . What a fuck fuck , god damn it fuck .',\n",
       " 'Yeah fuck that guy . Also , fuck you ! !',\n",
       " 'Fuck this guy in his stupid fuck face !']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94     22542\n",
      "           1       0.28      0.81      0.42      1132\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     23674\n",
      "   macro avg       0.63      0.85      0.68     23674\n",
      "weighted avg       0.96      0.89      0.92     23674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['toxic'].values, preds[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.to_csv(\"reddit_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[label_cols] = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['comment_text', 'toxic', 'toxic'+'_prob', 'toxic'+'_pred', 'len_comment']].iloc[\\\n",
    "    np.argwhere(preds[:,0] == 0).flatten().tolist()][test['toxic']==1].to_csv(\n",
    "    'reddit_toxic'+'_false_negative.csv')\n",
    "\n",
    "# .loc[test['toxic']==1].head()\n",
    "\n",
    "# [test['toxic']==1].to_csv(\n",
    "#     'reddit_toxic'+'_false_negative.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
